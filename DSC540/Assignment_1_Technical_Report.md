
Technical Report 

The article by Pearl discusses several different types, tasks, and tools of machine learning that are widely used in the academic and industry contexts. One form of learning they discuss is that of supervised machine learning, which is used for their analysis of reasons for applicant turnover at their previous job. In general, what this entails is the human begins by labeling every record by its classification, and then instructs the computer to detect the characteristics that lead to that classification. In the example of the education machine learning model from the paper by Sajjadiani et al., this means every listed reason for previous turnover is attached to each of the ~16,000 applicants in the dataset. The model then assesses the characteristics, based on the other features it has to go off of, and learns the patterns that lead to the classification. In their results, the authors note that involuntary turnover is inversely associated with performance and positively associated with voluntary turnover hazard.

As mentioned above, the paper by Sajjadiani et al. uses the task of classification with regards to how it determines the reason for the previous turnover in applicant resumes. And we also see that clustering is used, as the authors mention that the total years in the profession has been simplified into seven clusters. However, the larger model performs the task of classification, specifically using a Naïve Bayes Classifier, to predict the success of an application. This due to the fact that whether someone is offered a job in the school district is a binary outcome (a simple &quot;yes&quot; or &quot;no&quot;). Our feature data matrix also contains categories of text, further lending it to use of a classification method. The task of classification aims to use the feature characteristics to assign a value in the range of to determine the probability of a success. For each record, probabilities are calculated for the classes, where is the number of possible classifications. Whichever of these probabilities is the highest ends up being what the record is classified as by the model.

In the case of the Minneapolis education application dataset, the features include both the characteristics of the resume, as evaluated by a separate software, and special teacher evaluation metrics that are included with the application. The authors test several models, using varying combinations of features, to test both whether their approach is valid and which combinations yield the greatest predictive power. In all of them, the same Naïve Bayes approach is used to achieve a maximum probability of being hired by the district. The authors create a chart (Table 5) displaying the impact each feature has on the overall probability, and perhaps unsurprisingly prior district employment carries an extremely high weight in this calculation.

The article by Pearl also lists several tools for causal inference, and many are directly used in the paper by Sajjadiani et al. Perhaps most importantly, transparency and testability are listed first and the authors certainly use this. Their methodology is carefully explained, and they include both formulae specific for the task at hand as well as appendices describing some more basic details about the Naïve Bayes Classifier. The authors also use the fifth tool mentioned, which details adaptability, external validity, and sample selection bias. They note that while this could be applied to other educational boroughs, this analysis pertains to only one school district, and as such it also should warrant extreme caution when applying the methodology to other disciplines. They also address the sixth tool in the article, which deals with missing data. This is done for applicants who did not specify their gender or race, and accomplished by utilizing a reference database to classify those with missing values into male/female and white/nonwhite.

Another paper from the GCU library, that of Huang et al., uses another form of machine learning not discussed in the article by Pearl. This paper, which analyzes the designs of artificial bee colonies to look for faults, uses a method known as semi-supervised learning. As the name suggests, it combines the elements of supervised and unsupervised learning to create a hybrid system. Here, a small amount of data are labeled, and the machine trains itself on the characteristics associated with this subset in order to make predictions on the remaining unlabeled records. The paper separates the faults into categories and three cases, then uses machine learning to classify and predict the location of faults in the colony structures. The paper by Sajjadiani et al. could have also used this method, to see if it provided more accurate results. It does bring the strain of the task more towards equilibrium between human and computer, but can be used in many testing applications to see if the model is working properly. However, since the interest of the authors is seeing which features have the biggest effect on prediction, it appears the supervised model might have still been the better choice between these two for this application.

References:

Huang, J.-M., Wai, R.-J., &amp; Yang, G.-J. (2020). Design of Hybrid Artificial Bee Colony Algorithm and Semi-Supervised Extreme Learning Machine for PV Fault Diagnoses by Considering Dust Impact. _IEEE Transactions on Power Electronics_, _35_(7), 7086–7099. https://doi.org/10.1109/tpel.2019.2956812

Pearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. _Communications of the ACM_, _62_(3), 54–60. https://doi.org/10.1145/3241036

Sajjadiani, S., Sojourner, A. J., Kammeyer-Mueller, J. D., &amp; Mykerezi, E. (2019). Using machine learning to translate applicant work history into predictors of performance and turnover. _Journal of Applied Psychology_, _104_(10), 1207–1225. https://doi.org/10.1037/apl0000405
